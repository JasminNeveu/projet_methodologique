
\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true,
            linkcolor=black,
            citecolor=blue,
            urlcolor=blue]{hyperref}
\usepackage{caption}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\captionsetup[figure]{justification=centering}
\theoremstyle{plain}

\newtheorem{proposition}{Proposition}
\newcommand{\1}{\mathbf{1}}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]

\title{Methodological project report}
\author{Jasmin Neveu}
\date{\today}

\begin{document}



\begin{titlepage}
    \centering

    {\Large ENSAI \par}
    \vspace{2cm}
    {\large ATPA Track \par}
    {\large Academic Year 2024--2025 \par}
    \vspace{2cm}
    {\Huge\bfseries Methodological Project Report \par}
    \vspace{0.4cm}
    {\LARGE\itshape Selective Inference for Hierarchical Clustering \par}
    \vspace{2cm}
    \begin{flushleft}
        \textbf{Student:} Jasmin Neveu \\
        \vspace{0.2cm}
        \textbf{Supervisor:} Javier Gonzáles-Delgado
    \end{flushleft}
    \vspace{1cm}
    \begin{flushleft}
        \textbf{Analyzed article:} \\
        Lucy L. Gao, Jacob Bien, and Daniela Witten, \\
        \textit{Selective Inference for Hierarchical Clustering}, \\
        arXiv: 2012.02936v3 [stat.ME] 31 Oct 2022 \\
    \end{flushleft}
    \vspace{1cm}
    {\large Submission date: \today \par}
\end{titlepage}


\tableofcontents
\newpage

\section{Introduction}

Introduction text...

\section{Classical versus selective testing}

\subsection{Hypothesis testing and \textit{p-values}}

Let $(\Omega,\mathcal{F})$ and $(E,\mathcal{E})$ be a measurable spaces.
We denote by $\mathcal{M}_1$ the set of all probability measures on $(\Omega,\mathcal{F})$.
A random variable with values in $E$ is a measurable function $X: (\Omega,\mathcal{F},\mathbb{P}) \rightarrow (E,\mathcal{E})$, with $\mathbb{P} \in \mathcal{M}_{1}$

\begin{definition}[Hypothesis, 1.1 in~\cite{Ramdas}]\label{def_dominate_sto}
  A \textit{hypothesis} is a set of probability measures in $\mathcal{M}_1$.
  A hypothesis is \textit{simple} if it is a singleton, such as $\{\mathbb{P}\}$ or $\{\mathbb{Q}\}$.
  Otherwise it is \textit{composite}.
\end{definition}

\begin{remark}
  We define the alternative hypothesis to $\mathcal{H}_0$ as $\mathcal{M}_1 \setminus \mathcal{H}_0$.
\end{remark}

\begin{definition}[Test]\label{def_test}
  A \textit{test} is a function defined as
  \[
  \begin{array}{rcl}
    \pi : \mathcal{F} &\to& \{0,1\} \\
    x &\mapsto& \pi(x).
  \end{array}
  \]
\end{definition}

\begin{definition}[Type I error]\label{def_type_error}
  We say that a test controls the type I error at level $\alpha$ if
  \[
    \mathbb{P}(\text{Reject }\mathcal{H}_0) \leq \alpha, \quad \alpha \in (0,1).
  \]
  We say that it controls exactly the type I error at level $\alpha$ if
  \[
    \mathbb{P}(\text{Reject }\mathcal{H}_0) = \alpha.
  \]
\end{definition}

\begin{definition}[Stochastic dominance]\label{def_sto_def}
  Let $X$ and $Y$ be real-valued random variables.
  We say that $Y$ stochastically dominates $X$, and write
  \[
    X \preceq_{\mathrm{st}} Y,
  \]
  if
  \[
    \mathbb{P}(X \leq u) \geq \mathbb{P}(Y \leq u), \quad \forall u \in \mathbb{R}.
  \]
\end{definition}

\begin{definition}[Super uniform]\label{def_super_unif}
  Let $X$ be a real-valued random variable. We say that $X$ is super uniform, and write
  $X \sim SU(0,1)$, if $X$ stochastically dominates a uniform random
  variable on $[0,1]$, that is,
  \[
    \mathbb{P}(X \leq u) \leq u, \quad \forall u \in [0,1].
  \]
\end{definition}

\begin{proposition}\label{prop_repartition}
  Let $X$ and $Y$ be real-valued random variables, with distribution function $F_X$ and $F_Y$.
  If $X \preceq_{\mathrm{st}} Y$ then $F_X(Y) \sim SU(0,1)$.
\end{proposition}


\begin{proof}
Let \(G_X\) denote the generalized inverse (quantile function) of \(F_X\),
defined for \(u\in[0,1]\) by
\[
G_X(u) = \inf\{x\in\mathbb{R} : F_X(x) \ge u\}.
\]

By definition of the generalized inverse,
\[
\{F_X(Y) \le u\} = \{Y < G_X(u)\}.
\]
Therefore,
\[
\mathbb{P}\bigl(F_X(Y)\le u\bigr)
= \mathbb{P}\bigl(Y < G_X(u)\bigr)
= F_Y\bigl(G_X(u)^-\bigr),
\]
where \(G_X(u)^-\) denotes the left limit at \(G_X(u)\).

Since \(X \preceq_{\mathrm{st}} Y\), we have \(F_Y \le F_X\) pointwise, and thus
\[
F_Y\bigl(G_X(u)^-\bigr)
\le F_X\bigl(G_X(u)^-\bigr).
\]
By the defining property of the generalized inverse,
\[
F_X\bigl(G_X(u)^-\bigr) \le u.
\]

Combining these inequalities yields
\[
\mathbb{P}\bigl(F_X(Y)\le u\bigr) \le u,
\quad \forall u\in[0,1],
\]
which proves that \(F_X(Y)\) is super uniform.
\end{proof}


\begin{remark}\label{remark_FXX_unif}
If \(X=Y\), then \(F_X(X)\) is super uniform on \([0,1]\).  
Moreover, if \(X\) has a continuous distribution function \(F_X\), then
\[
F_X(X) \sim \mathcal{U}(0,1).
\]
\end{remark}

\begin{proof}
By the proposition \ref{prop_repartition}, we have that $F_X(X) \sim SU(0,1)$


If \(F_X\) is continuous, then \(F_X(G_X(u)) = u\) for all \(u\in[0,1]\), and hence
\[
\mathbb{P}\bigl(F_X(X)\le u\bigr)
= F_X\bigl(G_X(u)\bigr)
= u,
\quad \forall u\in[0,1].
\]
Therefore \(F_X(X)\sim \mathcal{U}(0,1)\) when \(X\) is continuously distributed.
\end{proof}


\begin{definition}[p-value, 1.1 in~\cite{Ramdas}]\label{def_pvalue}
  Let $\mathcal{H}_0$ be a hypothesis. A p-value for $\mathcal{H}_0$ is a super
  uniform random variable under $\mathcal{H}_0$. 
\end{definition}

\begin{proposition}
  Let $\mathcal{R}$ be the rejection rule defined by
  \[
    \mathcal{R} = \1_{\{p \leq \alpha\}}, \quad \alpha \in (0,1).
  \]
  Let $p$ be a \textit{p-value} for $\mathcal{H}_0$. Then $\mathcal{R}$
  controls the type I error at level $\alpha$.
\end{proposition}


\begin{proof}
By definition of the rejection rule,
\[
  \mathbb{P}_{\mathcal{H}_0}(Reject ~ \mathcal{H}_0)
= \mathbb{P}_{\mathcal{H}_0}(p \le \alpha).
\]
Since \(p\) is a p-value for \(\mathcal{H}_0\), it is super uniform under
\(\mathcal{H}_0\), hence
\[
\mathbb{P}_{\mathcal{H}_0}(p \le \alpha) \le \alpha.
\]
This establishes control of the type~I error at level \(\alpha\).
\end{proof}


For the rest of the document, we will only consider unilateral test.
Usually, we define the \textit{p-value} by 
\[
  p(X) = \mathbb{P}_{\mathcal{H}_0}(T(X) \geq t(x))
\]

But more generally, we will define \textit{p-value} with the following proposition.


\begin{proposition}\label{prop_test_stat_pvalue}
Let \(T\) and \(T'\) be two test statistics, i.e. transformations of $X$
\[
T : \mathcal{F} \to \mathbb{R},
\qquad
T' : \mathcal{F} \to \mathbb{R}.
\]
Let \(X\) be a random variable and \(x\) a realization of \(X\).
Define
\[
p(x) = \mathbb{P}_{\mathcal{H}_0}\!\bigl(T'(X) \ge T(x)\bigr).
\]
If
\[
T'(X) \preceq_{\mathrm{st}} T(X)
\quad \text{under } \mathcal{H}_0,
\]
then \(p\) is a p-value for \(\mathcal{H}_0\).
\end{proposition}

\begin{proof}
Under \(\mathcal{H}_0\), let \(F_{T'(X)}\) denote the distribution
function of \(T'(X)\).
By proposition~\ref{prop_repartition}, the stochastic dominance
\(T'(X) \preceq_{\mathrm{st}} T(X)\) implies that
\[
F_{T'(X)}\bigl(T(X)\bigr) \sim \mathrm{SU}(0,1).
\]

By definition,
\[
p(x)
= \mathbb{P}_{\mathcal{H}_0}\!\bigl(T'(X) \ge T(x)\bigr)
= 1 - F_{T'(X)}\bigl(T(x)\bigr).
\]

Let \(u \in [0,1]\). Then
\begin{align*}
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr)
&= \mathbb{P}_{\mathcal{H}_0}\!\left(1 - F_{T'(X)}(T(X)) \le u\right) \\
&= \mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \ge 1-u\right) \\
&= 1 - \mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \le 1-u\right).
\end{align*}

Since \(F_{T'(X)}(T(X))\) is super uniform,
\[
\mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \le 1-u\right) \le 1-u,
\]
and therefore
\[
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr) \le u.
\]

Thus \(p\) is super uniform under \(\mathcal{H}_0\), and hence a p-value.
\end{proof}

\begin{remark}\label{remark_equal_statistics}
If \(T' = T\) and the distribution function \(F_T\) of \(T(X)\) is continuous
under \(\mathcal{H}_0\), then
\[
p \;\overset{\mathcal{H}_0}{\sim}\; \mathcal{U}(0,1).
\]
\end{remark}

\begin{proof}
When \(T'=T\)
\[
p(X) = \mathbb{P}_{\mathcal{H}_0}\bigl(T(X') \ge T(X)\bigr)
      = 1 - F_T\bigl(T(X)\bigr),
\]

By remark~\ref{remark_FXX_unif}, if \(F_T\) is continuous,
\[
F_T\bigl(T(X)\bigr) \sim \mathcal{U}(0,1).
\]
Consequently, for any \(u\in[0,1]\),
\begin{align*}
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr)
&= \mathbb{P}_{\mathcal{H}_0}\bigl(1 - F_T(T(X)) \le u\bigr) \\
&= \mathbb{P}_{\mathcal{H}_0}\bigl(F_T(T(X)) \ge 1-u\bigr) \\
&= 1 - \mathbb{P}_{\mathcal{H}_0}\bigl(F_T(T(X)) \le 1-u\bigr) \\
&= 1 - (1-u) \\
&= u.
\end{align*}
Thus \(p\) is uniformly distributed on \([0,1]\) under \(\mathcal{H}_0\).
\end{proof}

\subsection{Examples motivating selective testing}




\subsubsection{Lasso}

To determine whether an explanatory variable helps explain a response variable through a linear regression model, one performs a significance test on the coefficient associated with that variable, testing whether it is significantly different from zero. In the context of classical linear regression, this falls within the framework of standard inference.

In contrast, Lasso regression uses an \(\ell_1\)-penalty to perform variable selection. Not all coefficients are tested—only those selected by the Lasso are considered. It is therefore impossible to define in advance which coefficients will be tested, since they depend on the outcome of the Lasso regression. This creates a situation of selective inference.

During the Algorithmic Programming course supervised by Brian Staber, we implemented a proximal gradient descent algorithm. We simulated a dataset as follows: let \(x_1, \dots, x_8\) be centered gaussian random variables with correlation matrix \((R_{ij})_{1\le i,j\le 8}\) defined by 
\[
R_{ij} = 0.5^{|i-j|}, \quad 1\le i,j\le 8.
\]
The response is modeled as
\[
y = \beta^\top x + 3 \varepsilon, \quad \varepsilon \sim \mathcal{N}(0,1), \quad \beta = (3,1.5,0,0,2,0,0,0)^\top.
\]
We generated a sample of size \(n=100\) and obtained the following regularization path.

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.7\textwidth]{images/lasso_path.png}
  \caption{Regularization path of the coefficients obtained using the Lasso algorithm.
Some coefficients are zero, and three converge toward the theoretical values 
(3,1.5,2).}
  \label{fig:lasso_path}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.7\textwidth]{images/pvalues_lasso.png}
  \caption{Cumulative distribution function of the p-values obtained from significance tests
of the third coefficient estimated by a Lasso regression. The distribution function
was computed using $M$ = 2000 simulations.}
  \label{fig:pvalue_lasso}
\end{figure}

\subsubsection{Clustering}



To illustrate the importance of using appropriate tests in the context of selective inference, I simulated samples of gaussian random variables to which I applied clustering methods (hierarchical clustering and \textit{k-means}).
I then performed tests of equality of means (\textit{z-tests}) on clusters chosen in a data-dependent manner.
If the test of equality of means correctly controlled the type I error in this setting, the resulting p-value would follow a uniform distribution on ([0,1]) when the null hypothesis is true.
In practice, however, the “clustering + post-selection testing” procedure leads to a deviation from the uniform distribution, demonstrating the lack of control of the type I error.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.45\textwidth]{images/cah_pvalues.png}%
  \hfill
  \includegraphics[width=0.45\textwidth]{images/kmeans_df.png}
  \caption{Cumulative distribution functions of the p-values obtained from tests of equality of means
    between clusters after hierarchical clustering (\textit{CAH}) and \textit{k-means} algorithms.
The distribution functions were computed using $M$ = 2000 simulations from a multivariate normal distribution with
  $\mu = 0_{n\times p}$ et $\Sigma = 0.98^{|i-j|}$ pour $1 \leq i,j \leq n \times p$, avec $n = 100$ et $p = 5$.}
  \label{fig:deux_images}
\end{figure}

\subsubsection{Publication bias}
\cite{fithian}
\subsection{Addressing selective testing}

\section{Post clustering inference}

\bibliographystyle{abbrv}
\bibliography{biblio.bib}

\end{document}
