
\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true,
            linkcolor=black,
            citecolor=blue,
            urlcolor=blue]{hyperref}
\usepackage{caption}
\usepackage{etoolbox}
\usepackage{dsfont} %for \mathds
\pretocmd{\section}{\clearpage}{}{}
\captionsetup[figure]{justification=centering}
\usepackage{subcaption}
\theoremstyle{plain}

\usepackage{changes}
\newcommand{\Javier}[1]{\textcolor{purple}{#1}}

\newcommand{\1}{\mathbf{1}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{assumption}{Assumption}[section]


\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\title{Mmethodological project report}
\author{Jasmin Neveu}
\date{\today}

\begin{document}



\begin{titlepage}
    \centering

    {\Large ENSAI \par}
    \vspace{2cm}
    {\large ATPA Track \par}
    {\large Academic Year 2024--2025 \par}
    \vspace{2cm}
    {\Huge\bfseries Methodological Project Report \par}
    \vspace{0.4cm}
    {\LARGE\itshape Selective Inference for Hierarchical Clustering \par}
    \vspace{2cm}
    \begin{flushleft}
        \textbf{Student:} Jasmin Neveu \\
        \vspace{0.2cm}
        \textbf{Supervisor:} Javier González-Delgado
    \end{flushleft}
    \vspace{1cm}
    \begin{flushleft}
        \textbf{Analyzed article:} \\
        Lucy L. Gao, Jacob Bien, and Daniela Witten, \\
        \textit{Selective Inference for Hierarchical Clustering}, \\
        Journal of the American Statistical Association,\\ 119(545), 332–342, 2024.\\
    \end{flushleft}
    \vspace{1cm}
    {\large Submission date: \today \par}
\end{titlepage}


\tableofcontents
\newpage

\section{Introduction}

Introduction text...

\section{Classical versus selective testing}\label{sec:testing}

\subsection{Hypothesis testing and \textit{p-values}}

\Javier{\textbf{General comments}: This section is well-written and structured, but some work needs to be done. I have added some main comments about how to present some of the objects. We will discuss about that. Once this is done, we will speak about improving the flow by adding some text that helps the reader and creates a `story'.}

\Javier{As we have a 20 pages limit we will probably have to move the proofs to the appendix (which is the usual practice in research articles). I have added an appendix at the end where you can move the proofs. Then, we will mention in the text that proofs are provided in the Appendix.}

\Javier{Minor comment: to write equations, use \begin{equation}\label{my-equation}
2+2
\end{equation} so that equation numbers appear in the text and equations can be referenced therein, using ~\eqref{my-equation}. If you don't want an equation to be numbered, because it maybe not be very relevant, or it corresponds to calculations inside a proof, use 
\begin{equation*}
1+1.
\end{equation*}
}
%
% Let $(\Omega,\mathcal{F})$ and $(E,\mathcal{E})$ be a measurable spaces.
% We denote by $\mathcal{M}_1$ the set of all probability measures on $(\Omega,\mathcal{F})$.
% A random variable with values in $E$ is a measurable function $X: (\Omega,\mathcal{F},\mathbb{P}) \rightarrow (E,\mathcal{E})$, with $\mathbb{P} \in \mathcal{M}_{1}$
%
\Javier{I think it is better is presented as follows (we will discuss next time about this). The main point is that I think is better to work directly on $E$ (the topological space where the random variable takes values) for clarity. We will clarify next time.}

Let $(\Omega, \mathcal{F},\mathbb{P})$ be a probability space, $(E,\mathcal{E})$ a topological space and $\mathcal{T}$ the $\sigma$-algebra generated by $\mathcal{E}$. A \textit{random variable} is a measurable function $X:(\Omega,\mathcal{F})\rightarrow(E,\mathcal{T})$. The \textit{(probability) distribution} of $X$ is the mapping $P:\mathcal{T}\rightarrow[0,1]$ such that  $P(O) = (\mathbb{P}\circ X^{-1})(O)$ for all $O\in\mathcal{T}$. We say that $P$ is \textit{supported} on $E$ and denote by $\mathcal{M}_1(E)$ the set of all probability distributions supported on $\mathcal{E}$. From now on, we will set $E=\mathbb{R}$ and simply write $\mathcal{M}_1(\mathbb{R})=\mathcal{M}_1$.

\Javier{Probably the previous paragraph should be formulated more in detail, especially when defining $\mathcal{M}_1$. To discuss.}

\begin{definition}[Hypothesis, Definition 1.1 in~\cite{Ramdas}]\label{def_dominate_sto}
  A \textit{hypothesis} \added{$\mathcal{H}_0$} is a set of probability distributions in $\mathcal{M}_1$.
  A hypothesis is \textit{simple} if it is a singleton, such as $\lbrace P \rbrace$ or $\lbrace Q \rbrace$, and \textit{composite} otherwise. The complementary set $\mathcal{M}_1 \setminus \mathcal{H}_0$ is called its \textit{alternative hypothesis}.
\end{definition}

\begin{definition}[Test]\label{def_test}
  A \textit{test} for $\mathcal{H}_0$ is a binary partition of the sample space, defined by a mapping function.
 \begin{equation}
  \begin{array}{rcl}
    \pi_{\mathcal{H}_0} : \Omega &\to& \{0,1\} \\
    \omega &\mapsto& \pi_{\mathcal{H}_0}(\omega).
  \end{array}
 \end{equation}
 For $\omega \in \Omega$, we say that the test \textit{rejects}
  $\mathcal H_0$ based on $\omega$ if $\pi_{\mathcal{H}_0}(\omega)=1$, and \textit{does not reject}
  $\mathcal H_0$ based on $\omega$ if $\pi_{\mathcal{H}_0}(\omega)=0$.
\end{definition}

%TODO: bien montrer que pi dépends de H_0, mais jsp comment l'écrire

\Javier{I have moved the `rejection' definition inside the test one. I think this is clearer. Then, no need to speak of \textit{rejection rule} but only of \textit{test} (equivalent concepts). I know I spoke about rejection rule but I think this is clearer.}


\Javier{In what follows, we will be writing $\mathbb{P}(\text{Reject } \mathcal{H}_0)$, which is a standard and easy-to-read form. Before starting using that, you should write a note explaning that this is a notation that you will be adopting from now on, meaning:
\begin{equation*}
\mathbb{P}\left(\text{Reject } \mathcal{H}_0\right) = \mathbb{P}\left(\lbrace \omega \in \Omega\,:\,\pi_{\mathcal{H}_0}(\omega)=1\rbrace\right),
\end{equation*}
according to your previous definition. You can add that we make the dependence on $\pi$ implicit. In short: it is okay to use shortcuts as `Reject H0' but we always need to formally specify what we mean by them.
}

\Javier{Don't use italics for \textit{Reject} in equations.}

From now, we will use the following notation:
\begin{equation*}
\mathbb{P}\left(\text{Reject } \mathcal{H}_0\right) = \mathbb{P}\left(\lbrace \omega \in \Omega\,:\,\pi_{\mathcal{H}_0}(\omega)=1\rbrace\right),
\end{equation*}
The dependence on $\pi$ is implicit.



\begin{definition}[Type I error]\label{def_type_error}
  We say that a test controls the type I error at level $\alpha$ if
  \[
    \mathbb{P}(\text{Reject }\mathcal{H}_0) \leq \alpha, \quad \text{for }\alpha \in (0,1).
  \]
  We say that it controls  the type I error exactly at level $\alpha$ if
  \[
    \mathbb{P}(\text{Reject }\mathcal{H}_0) = \alpha, \quad\text{for } \alpha \in (0,1).
  \]
\end{definition}

\begin{definition}[Stochastic dominance]\label{def_sto_def}
  Let $X$ and $Y$ be real-valued random variables.
  We say that $Y$ stochastically dominates $X$, and write
  \[
    X \preceq_{\mathrm{st}} Y,
  \]
  if
  \[
    \mathbb{P}(X \leq u) \geq \mathbb{P}(Y \leq u), \quad \forall\, u \in \mathbb{R}.
  \]
\end{definition}

\Javier{Note: Use $\mathcal{SU}(0,1)$ instead of $SU(0,1)$!}

\begin{definition}[Super\added{-}uniform random variable]\label{def_super_unif}
  Let $X$ be a real-valued random variable. We say that $X$ is super\added{-}uniform, and write
  $X \sim \mathcal{SU}(0,1)$, if $X$ stochastically dominates a uniform random
  variable on $[0,1]$, that is, if
  \[
    \mathbb{P}(X \leq u) \leq u, \quad \forall\, u \in [0,1].
  \]
\end{definition}

\begin{proposition}\label{prop_repartition}
  Let $X$ and $Y$ be real-valued random variables, with cumulative distribution functions $F_X$ and $F_Y$, respectively.
  If $X \preceq_{\mathrm{st}} Y$, then $F_X(Y) \sim SU(0,1)$.
\end{proposition}


\begin{remark}\label{remark_FXX_unif}
If \(X=Y\), then \(F_X(X)\) is super-uniform on \([0,1]\).  
Moreover, if \(X\) has a continuous distribution function \(F_X\), then
\[
F_X(X) \sim \mathcal{U}(0,1).
\]
\end{remark}

\Javier{Note: Write $p$-value instead of p-value!}

\begin{definition}[$p$-value, Definition 1.1 in~\cite{Ramdas}]\label{def_pvalue}
  Let $\mathcal{H}_0$ be a hypothesis. A $p$-value for $\mathcal{H}_0$ is a super-uniform random variable under $\mathcal{H}_0$. 
\end{definition}

We often build tests using $p$-values as $\pi = \mathds{1}\{p \leq \alpha \}$.

\Javier{Not amazing to start a sentence with the word $p$-value. Also you can use $\mathds{1}$ for the indicator function.}

\Javier{The previous paragraph is okay, but (following my previous comment) I think we should only speak about \textit{test} and avoid \textit{rejection rule}. So you can say that we often build tests using $p$-values as $\pi=\mathds{1}\lbrace p \leq \alpha\rbrace$. So in the following proposition you can also replace $\mathcal{R}$ by $\pi$.}

\begin{proposition}\label{prop_rejection_rule}
  Let $\mathcal{H}_0$ be a hypothesis, $p$ a $p$-value for $\mathcal{H}_0$ and $\mathcal{R}$ the rejection rule defined by
  \[
    \pi = \mathds{1}{\{p \leq \alpha\}}, \quad \alpha \in (0,1).
  \]
 Then, $\mathcal{R}$
  controls the type I error at level $\alpha$. 
\end{proposition}

\begin{definition}[Test statistic]
  A test statistic is a measurable function $T : \Omega \to \mathbb{R}$.
\end{definition}


From now on, we will consider the case of unilateral tests.
In this setting, to define a test based on the information given by a real-valued random variable $X$, $p$-values are foten built in the form: 
\begin{equation}\label{unilateral_uniform_pvalue}
  p(x) = \mathbb{P}_{H_0}(T(X) \geq T(x))
\end{equation}
where $T$ is a test statistic and $x$ a realization of $X$.

The $p$-value \eqref{unilateral_uniform_pvalue} follow a uniform law under $\mathcal{H}_0$. However we have defined $p$-values as being $SU$, which is a more general family of distributions. To adapt the form of the $p$-value to that setting, we adopt the construction of the following proposition.

\Javier{The previous sentence does not clearly justify why do we propose the following proposition. I would recall the following: 1. The $p$-value~\eqref{unilateral_uniform_pvalue} has a uniform distribution under $\mathcal{H}_0$. 2. However, we have defined $p$-values as being SU, which is a more general family of distributions. 3. To adapt the form of the $p$-value to that setting, we adopt the construction of the following proposition.}

\begin{proposition}\label{prop_test_stat_pvalue}
Let \(T\) and \(T'\) be two test statistics. Let \(X\) be a random variable and \(x\) a realization of \(X\).
Define
\[
p(x) = \mathbb{P}_{\mathcal{H}_0}\!\bigl(T'(X) \ge T(x)\bigr).
\]
If
\[
T'(X) \preceq_{\mathrm{st}} T(X)
\quad \text{under } \mathcal{H}_0,
\]
then \(p\) is a $p$-value for \(\mathcal{H}_0\).
\end{proposition}
\begin{remark}\label{remark_equal_statistics}
If \(T' = T\) and the distribution function \(F_T\) of \(T(X)\) is continuous
under \(\mathcal{H}_0\), then
\[
p \;\overset{\mathcal{H}_0}{\sim}\; \mathcal{U}(0,1).
\]
\end{remark}


In the classical setting, the null hypothesis is independent of the data . However, in many practical applications  $\mathcal{H}_0$ is chosen \textit{after seeing the data}. In this setting,  the classical testing approaches built for type I error control are unsuitable. Instead, statistical guarantees need to be provided via \textit{selective inference}. In particular, the theory of statistical testing of data-driven null hypotheses is known as \textit{selective testing}\cite{fithian}. The next section provides some examples that motivate this framework. 
\subsection{Examples motivating selective testing}

\subsubsection{Lasso}

To determine whether an explanatory variable helps explain a response variable through a linear regression model, a common practice is to test whether its associated coefficient is significantly different from zero.
In the context of classical linear regression, this falls within the framework of non-selective inference. In contrast, Lasso regression uses an \(\ell_1\)-penalty to perform variable selection \Javier{cite the Lasso paper}.
As only the coefficients selected by the Lasso can be tested, the null hypothesis depends on the outcome of the Lasso regression, and is therefore data-driven. This corresponds to a setting of selective inference, where the classical control of type I error fails. To illustrate so, consider a centered Gaussian vector $X=(X_1,\ldots,X_8)$.
\Javier{Standard notation: upper case for random variables and lower case for their realizations. Also, Gaussian is written in upper case.}
% \[
% R_{i,j} = 0.5^{|i-j|}, \quad 1\le i,j\le 8.
% \]
The response is modeled as
\[
Y = X \beta +  \varepsilon, \quad \varepsilon \sim \mathcal{N}(0,1), \quad \beta = (3,1.5,0,0,2,0,0,0)^\top.
\]
\Javier{Is $\varepsilon$ one-dimensional?}

We generated $n=100$ independent realizations of $(X,Y)$ and implemented the Lasso algorithm, obtaining the regularization path presented in Figure~\ref{fig:lasso_path}(a).
Then, we tested whether a randomly chosen coefficient selected by Lasso regression equals 0.
After repeating this pipeline $M=2000$ times, we obtained the empirical $p$-value distribution depicted in Figure \ref{fig:lasso_path}(b).
We clearly see that the $p$-values are not super-uniform, therefore a selective inference approach should be used instead of naive testing after selection. \Javier{Avoid starting a sentence with the word $p$-value.}


\Javier{Advice: use two panels in the same figure. I have modified it but you can undo if you don't like.}

\Javier{I have modified a bit the text above but it is still not very clear. The simulation needs to be explained more clearly. One question: the coefficient $\beta_3$ is always selected across the $M$ simulations?}

\begin{figure}[H]
  \centering
  \begin{subfigure}{0.47\textwidth}
  \includegraphics[width = \textwidth]{images/lasso_path.png} 
	\caption{}
 \end{subfigure}
 	 \begin{subfigure}{0.47\textwidth}
 	 \includegraphics[width = \textwidth]{images/lasso_pvalues.png}
  \caption{}
  \end{subfigure}
  \caption{(a) Regularization path of the coefficients obtained using the Lasso algorithm. \Javier{This is for a single realization of $(X,Y)$, right?}
Some coefficients are zero, and three converge toward the theoretical values 
(3,1.5,2). (b) Empirical cumulative distribution function (ECDF) of the $p$-values obtained from tesing wheter a randomly chosen coefficient is null after selecting the coefficient by a Lasso regression. The ECDF was computed from $M$ = 2000 simulations.}
  \label{fig:lasso_path}
\end{figure}


%\begin{figure}[H]
%  \centering
%  \includegraphics[width = 0.7\textwidth]{images/pvalues_lasso.png}
%  \caption{Cumulative distribution function of the $p$-values obtained from significance tests
%of the third coefficient estimated by a Lasso regression. The empirical distribution function
%was computed using $M$ = 2000 simulations.}
%  \label{fig:pvalue_lasso}
%\end{figure}


\subsubsection{Publication bias}

Most published studies gain publication due to their demonstrated significance.  
Only studies presentating major results are published by scientifics.
Thus, there is a selection process.
If $Y_i \sim \mathcal{N}(\mu_i,1)$ represents the effect size of a scientific study and only those with $|Y_i| > 1$ are published, denoted $\hat{I} = \{i: |Y_i| > 1\}$, then a naive level $\alpha$ test $H_{0,i}: \mu_i = 0$ for $i \in \hat{I}$ is invalid.
Indeed, Fithian \cite{fithian} demonstrates that the false positive rate among true nulls reaches approximately 0.16, far exceeding the nominal 0.05 level.  
Valid inference requires thresholding $|Y_i|$ at 2.41 rather than 1.96, the $0.95$ quantile of the standard normal, imposing a more stringent criterion.

\Javier{To discuss together!}

\subsubsection{Clustering}

Another remarkable example of selective inference appears when evaluating the performance of clustering algorithms by testing for the equality of cluster means.
To illustrate the need of using appropriate tests in the context of selective inference, we simulated $n = 1000$ samples of a centered Gaussian random variable \Javier{Not clear! Gaussian random vectors defined how? And how many samples?} that was classified into $K = 3$ groups using hierarchical clustering and $k$-means.

For each sample, the equality of cluster means was tested using a classical t-test, for two randomly selected clusters. If the test controlled the type I error in this setting, the resulting $p$-value would be super-uniformly distributed under the null hypothesis. 
However, the `clustering + post-selection testing' procedure leads to a deviation from super-uniformity, as shown in Figure~\ref{fig:deux_images}.


\begin{figure}[H]
 
 \begin{subfigure}{0.47\textwidth}
  \centering
  \includegraphics[width=\textwidth]{images/cah_pvalues.png}%
  \caption{}
\end{subfigure}  
  \hfill
  \begin{subfigure}{0.47\textwidth}
  \includegraphics[width=\textwidth]{images/kmeans_pvalues.png}
	\caption{}  
  \end{subfigure}
  \caption{Cumulative distribution functions of the $p$-values obtained from tests of equality of means
    between clusters after (a) hierarchical agglomerative clustering (HAC) and (b) $k$-means algorithms.
The distribution functions were computed using $M$ = 2000 simulations from a univariate centered normal distribution.
}
  \label{fig:deux_images}
\end{figure}

\subsection{Addressing selective testing}

Since the type I error is no longer controlled, alternative approaches are required.
Accordingly, we present three methods as described by Yoav Benjamini \cite{Benjamini}.

\subsubsection{Simultaneous Inference}

This approach controls the family-wise error rate across all hypotheses: 
\[
\mathbb{P}(\text{At least 1 false positive among all hypotheses}) \leq \alpha.
\]

This strategy proves highly conservative, ensuring that for every possible set of hypotheses, the probability of at least one false positive remains below $\alpha$.
\subsubsection{Sample splitting}

This approach consists in splitting the dataset into a training set $X$ and a test set $Y$.  
The training set $X$ is used to choose which hypotheses to test, denoted $H_0(X)$.  
Then, the tests are performed on the test set $Y$.

Although relatively simple to implement, this method raises several issues.  
First, statistical guarantees on the tests hold only if $X$ and $Y$ are independent, which is rarely the case in practice.  
In addition, comparing cluster means on the observations in $Y$ requires assigning each test point to one of the clusters obtained from the clustering performed on $X$, a step that compromises validity.
As discussed in \cite{Gao}, this strategy does not yield valid post-clustering inference in general.


\subsubsection{Conditional Inference}

This approach constitutes the most extensively studied framework for post-clustering inference.  
It controls the false positive rate conditional on hypothesis selection:  
\[
\mathbb{P}(\text{Reject } H_0(X) \mid H_0 \text{ selected}) \leq \alpha.
\]

In the remainder of the article, we employ this method to develop a statistical procedure for testing equality of cluster means following a clustering algorithm.

\Javier{Why new page?}

\section{Post clustering inference}\label{sec:post_clustering_inf}



\subsection{Notation and preliminaries}

Let $\mathbf{X} \in \mathbb{R}^{n \times p}$ be the design matrix.
A cluster is an element of a partition of the samples.
We note $C$ a clustering algorithm and $C_1 \in C(\mathbf{X})$ a cluster obtained by the algorithm $C$ on $\mathbf{X}$.

We note $\bm{\mu} = (\bm{\mu}_{ij})_{ij}$ such as $\bm{\mu}_{ij} = \mathbb{E}[\mathbf{X}_{ij}]$, with $\mathbf{X}_{ij}$ the element in row i and column j of the matrix $\mathbf{X}$.
For a subset $G$ of $\{1,\ldots,n\}$, we note $\overline{\bm{\mu}}_G = \frac{1}{|G|} \displaystyle\sum_{i \in G} \mu_i \in \mathbb{R}^p$ and $\overline{\mathbf{X}}_G = \frac{1}{|G|} \displaystyle\sum_{i \in G} X_i \in \mathbb{R}^p$

\begin{definition}[Null Hypothesis]
  \[H_0^{\{C_1,C_2\}}: \overline{\bm{\mu}}_{C_1(X)} = \overline{\bm{\mu}}_{C_2(X)}\]
\end{definition}

\begin{definition}[Type I selective error for clustering]
  We say that a test controls the type I selective error for clustering at leval $\alpha$ if
  \[
    \mathbb{P}_{H_0^{\{C_1,C_2\}}}(\textup{Reject} ~ H_0^{\{C_1,C_2\}} ~ | ~ C_1,C_2 \in C(\mathbf{X})) \leq \alpha,\quad \alpha \in (0,1).
  \]
  We say that it constrols exactly the type I selective error for clustering at level $\alpha$ if
\[
  \mathbb{P}_{H_0^{\{C_1,C_2\}}}(\textup{Reject} ~ H_0^{\{C_1,C_2\}} ~ | ~ C_1,C_2 \in C(\mathbf{X})) = \alpha, \quad \alpha \in (0,1).
  \]

\end{definition}

%TODO: Gérer problème d'alinéa

To run a proper test, we need to control this error at level $\alpha$.
In the ideal, we would like to define a p-value as following:
\[
  p_{ideal}(x) = \mathbb{P}_{H_0^{\{C_1,C_2\}}}(T(\mathbf{X}) \geq T(x) ~ | ~ C_1,C_2 \in C(\mathbf{X}))
\]
with $T$ being a test statistic.

With this p-value, we can control the selective type I error for clustering.

\begin{proposition}\label{prop_pideal}
  The selection rule $\pi = \mathds{1}{\{p_{ideal} \leq \alpha\}}, \quad \alpha \in (0,1).$ controls the selective type I error for clustering at leval $\alpha$
\end{proposition}

%TODO: proof that p_ideal is a p-value
However, $p_{ideal}$ cannot be evaluated in pratice as it depends on parameters that are unknown. 
Thus, to address this issue, we need to add technical events to the conditioning set and consided:
\[
  p(x) = \mathbb{P}_{H_0^{\{C_1,C_2\}}}(T(\mathbf{X}) \geq T(x) ~ | ~ C_1,C_2 \in C(\mathbf{X}), E[\mathbf{X}])
\]

\subsection{Gao et al. approach}

\appendix

\section{Proofs}

\subsection{Proofs of Section~\ref{sec:testing}}


\begin{proof}[Proof of Proposition~\ref{prop_repartition}]

Let \(G_X\) denote the generalized inverse (quantile function) of \(F_X\),
defined for \(u\in[0,1]\) as
\[
G_X(u) = \inf\{x\in\mathbb{R} : F_X(x) \ge u\}.
\]

By definition of the generalized inverse,
\[
\{F_X(Y) \le u\} = \{Y < G_X(u)\}.
\]
Therefore,
\[
\mathbb{P}\bigl(F_X(Y)\le u\bigr)
= \mathbb{P}\bigl(Y < G_X(u)\bigr)
= F_Y\bigl(G_X(u)^-\bigr),
\]
where \(G_X(u)^-\) denotes the left limit at \(G_X(u)\).

Since \(X \preceq_{\mathrm{st}} Y\), we have \(F_Y \le F_X\) pointwise, and thus
\[
F_Y\bigl(G_X(u)^-\bigr)
\le F_X\bigl(G_X(u)^-\bigr).
\]
By the defining property of the generalized inverse,
\[
F_X\bigl(G_X(u)^-\bigr) \le u.
\]

Combining these inequalities yields
\[
\mathbb{P}\bigl(F_X(Y)\le u\bigr) \le u,
\quad \forall\, u\in[0,1],
\]
which proves that \(F_X(Y)\) is super-uniform.


\end{proof}


\begin{proof}[Proof of Remark~\ref{remark_FXX_unif}]

By the proposition \ref{prop_repartition}, we have that $F_X(X) \sim SU(0,1)$. If \(F_X\) is continuous, then \(F_X(G_X(u)) = u\) for all \(u\in[0,1]\), and hence
\[
\mathbb{P}\bigl(F_X(X)\le u\bigr)
= F_X\bigl(G_X(u)\bigr)
= u,
\quad \forall\,u\in[0,1],
\]
which concludes the proof. \Javier{It's okay, but no need to repeat what we are proving at the end.}

\end{proof}



\begin{proof}[Proof of Proposition~\ref{prop_rejection_rule}]

By definition of the rejection rule,
\[
  \mathbb{P}_{\mathcal{H}_0}(\text{Reject} ~ \mathcal{H}_0)
= \mathbb{P}_{\mathcal{H}_0}(p \le \alpha).
\]
Since \(p\) is a $p$-value for \(\mathcal{H}_0\), it is super-uniform under
\(\mathcal{H}_0\), hence
\[
\mathbb{P}_{\mathcal{H}_0}(p \le \alpha) \le \alpha.
\]
This establishes control of the type~I error at level \(\alpha\).
\end{proof}





\begin{proof}[Proof of Proposition~\ref{prop_test_stat_pvalue}]

Let \(F_{T'(X)}\) denote the distribution
function of \(T'(X)\) under $\mathcal{H}_0$.
By Proposition ~\ref{prop_repartition}, the stochastic dominance
\(T'(X) \preceq_{\mathrm{st}} T(X)\) implies that
\[
F_{T'(X)}\bigl(T(X)\bigr) \sim \mathrm{SU}(0,1).
\]

By definition,
\[
p(x)
= \mathbb{P}_{\mathcal{H}_0}\!\bigl(T'(X) \ge T(x)\bigr)
= 1 - F_{T'(X)}\bigl(T(x)\bigr).
\]

Let \(u \in [0,1]\). Then
\begin{align*}
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr)
&= \mathbb{P}_{\mathcal{H}_0}\!\left(1 - F_{T'(X)}(T(X)) \le u\right) \\
&= \mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \ge 1-u\right) \\
&= 1 - \mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \le 1-u\right).
\end{align*}

Since \(F_{T'(X)}(T(X))\) is super-uniform,
\[
\mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \le 1-u\right) \le 1-u,
\]
and therefore
\[
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr) \le u.
\]

Thus\added{,} \(p\) is super-uniform under \(\mathcal{H}_0\), and hence a $p$-value.
\end{proof}


\begin{proof}[Proof of Remark~\ref{remark_equal_statistics}]

When \(T'=T\)
\[
p(X) = \mathbb{P}_{\mathcal{H}_0}\bigl(T(X') \ge T(X)\bigr)
      = 1 - F_T\bigl(T(X)\bigr),
\]

By Remark ~\ref{remark_FXX_unif}, if \(F_T\) is continuous,
\[
F_T\bigl(T(X)\bigr) \sim \mathcal{U}(0,1).
\]
Consequently, for any \(u\in[0,1]\),
\begin{align*}
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr)
&= \mathbb{P}_{\mathcal{H}_0}\bigl(1 - F_T(T(X)) \le u\bigr) \\
&= \mathbb{P}_{\mathcal{H}_0}\bigl(F_T(T(X)) \ge 1-u\bigr) \\
&= 1 - \mathbb{P}_{\mathcal{H}_0}\bigl(F_T(T(X)) \le 1-u\bigr) \\
&= 1 - (1-u) \\
&= u.
\end{align*}
Thus \(p\) is uniformly distributed on \([0,1]\) under \(\mathcal{H}_0\).
\end{proof}


\subsection{Proofs of Section~\ref{sec:post_clustering_inf}}

\begin{proof}[Proof of Proposition~\ref{prop_pideal}]
  ...
\end{proof}



\bibliographystyle{abbrv}
\bibliography{biblio.bib}

\end{document}
