
\documentclass[a4paper,11pt]{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true,
            linkcolor=black,
            citecolor=blue,
            urlcolor=blue]{hyperref}
\usepackage{caption}
\usepackage{etoolbox}
\pretocmd{\section}{\clearpage}{}{}
\captionsetup[figure]{justification=centering}
\theoremstyle{plain}

\usepackage{changes}
\newcommand{\Javier}[1]{\textcolor{purple}{#1}}

% \newtheorem{proposition}{Proposition}
\newcommand{\1}{\mathbf{1}}
% \newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
% \newtheorem{prop}[theorem]{Proposition}
% \newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]


\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\title{Mmethodological project report}
\author{Jasmin Neveu}
\date{\today}

\begin{document}



\begin{titlepage}
    \centering

    {\Large ENSAI \par}
    \vspace{2cm}
    {\large ATPA Track \par}
    {\large Academic Year 2024--2025 \par}
    \vspace{2cm}
    {\Huge\bfseries Methodological Project Report \par}
    \vspace{0.4cm}
    {\LARGE\itshape Selective Inference for Hierarchical Clustering \par}
    \vspace{2cm}
    \begin{flushleft}
        \textbf{Student:} Jasmin Neveu \\
        \vspace{0.2cm}
        \textbf{Supervisor:} Javier González-Delgado
    \end{flushleft}
    \vspace{1cm}
    \begin{flushleft}
        \textbf{Analyzed article:} \\
        Lucy L. Gao, Jacob Bien, and Daniela Witten, \\
        \textit{Selective Inference for Hierarchical Clustering}, \\
        Journal of the American Statistical Association,\\ 119(545), 332–342, 2024.\\
    \end{flushleft}
    \vspace{1cm}
    {\large Submission date: \today \par}
\end{titlepage}


\tableofcontents
\newpage

\section{Introduction}

Introduction text...

\section{Classical versus selective testing}\label{sec:testing}

\subsection{Hypothesis testing and \textit{p-values}}

\Javier{\textbf{General comments}: This section is well-written and structured, but some work needs to be done. I have added some main comments about how to present some of the objects. We will discuss about that. Once this is done, we will speak about improving the flow by adding some text that helps the reader and creates a `story'.}

\Javier{As we have a 20 pages limit we will probably have to move the proofs to the appendix (which is the usual practice in research articles). I have added an appendix at the end where you can move the proofs. Then, we will mention in the text that proofs are provided in the Appendix.}

\Javier{Minor comment: to write equations, use \begin{equation}\label{my-equation}
2+2
\end{equation} so that equation numbers appear in the text and equations can be referenced therein, using ~\eqref{my-equation}. If you don't want an equation to be numbered, because it maybe not be very relevant, or it corresponds to calculations inside a proof, use 
\begin{equation*}
1+1.
\end{equation*}
Also, Definitions and Remarks are numbered like 2.1, 2.2 but Propositions like 1,2,3... The same numbering should be used for all.\\
}
%
% Let $(\Omega,\mathcal{F})$ and $(E,\mathcal{E})$ be a measurable spaces.
% We denote by $\mathcal{M}_1$ the set of all probability measures on $(\Omega,\mathcal{F})$.
% A random variable with values in $E$ is a measurable function $X: (\Omega,\mathcal{F},\mathbb{P}) \rightarrow (E,\mathcal{E})$, with $\mathbb{P} \in \mathcal{M}_{1}$
%
\Javier{I think it is better is presented as follows (we will discuss next time about this). The main point is that I think is better to work directly on $E$ (the topological space where the random variable takes values) for clarity. We will clarify next time.}

Let $(\Omega, \mathcal{F},\mathbb{P})$ be a probability space, $(E,\mathcal{E})$ a topological space and $\mathcal{T}$ the $\sigma$-algebra generated by $\mathcal{E}$. A \textit{random variable} is a measurable function $X:(\Omega,\mathcal{F})\rightarrow(E,\mathcal{T})$. The \textit{(probability) distribution} of $X$ is the mapping $P:\mathcal{T}\rightarrow[0,1]$ such that  $P(O) = (\mathbb{P}\circ X^{-1})(O)$ for all $O\in\mathcal{T}$. We say that $P$ is \textit{supported} on $E$ and denote by $\mathcal{M}_1(E)$ the set of all probability distributions supported on $\mathcal{E}$. From now on, we will set $E=\mathbb{R}$ and simply write $\mathcal{M}_1(\mathbb{R})=\mathcal{M}_1$.

\Javier{Probably the previous paragraph should be formulated more in detail, especially when defining $\mathcal{M}_1$. To discuss.}

\begin{definition}[Hypothesis, Definition 1.1 in~\cite{Ramdas}]\label{def_dominate_sto}
  A \textit{hypothesis} is a set of probability distributions in $\mathcal{M}_1$.
  A hypothesis is \textit{simple} if it is a singleton, such as $\lbrace P \rbrace$ or $\lbrace Q \rbrace$.
  Otherwise it is \textit{composite}.

  We define the alternative hypothesis to $\mathcal{H}_0$ as $\mathcal{M}_1 \setminus \mathcal{H}_0$.
\end{definition}

\begin{definition}[Test]\label{def_test}
  A \textit{test} is a function defined as
  \[
  \begin{array}{rcl}
    \pi : \Omega &\to& \{0,1\} \\
    \omega &\mapsto& \pi(\omega).
  \end{array}
  \]
\end{definition}

%TODO: bien montrer que pi dépends de H_0, mais jsp comment l'écrire
\begin{definition}[Rejection rule]

  Let $\mathcal H_0 \subset \mathcal M_1$ be a hypothesis
  and let $\pi : \Omega \to \{0,1\}$ be a test.
  For $\omega \in \Omega$, we say that the test \emph{rejects}
  $\mathcal H_0$ if $\pi(\omega)=1$, and \emph{does not reject}
  $\mathcal H_0$ if $\pi(\omega)=0$.
\end{definition}



\Javier{The test function is defined on $\Omega$ (it makes a decision based on an observation of the sample space, not based on a measurable set!). In the following definition you speak about rejection of $\mathcal{H}_0$ but this has not been defined. There has to be a link between the definition of test function (if defined and used) and the concept of rejecting/accepting a hypothesis. \textbf{Importantly}: You have defined a (simple) hypothesis as a distribution $\lbrace P \rbrace$. Then, what do we mean by \textit{rejecting} a hypothesis? This should be defined (no need to define a test function for that if you don't want to). We need to define rejection of $\mathcal{H}_0$ before speaking about type I error. If this is not clear we will discuss next time.}

\begin{definition}[Type I error]\label{def_type_error}
  We say that a test controls the type I error at level $\alpha$ if
  \[
    \mathbb{P}(\text{Reject }\mathcal{H}_0) \leq \alpha, \quad \text{for }\alpha \in (0,1).
  \]
  We say that it controls  the type I error exactly at level $\alpha$ if
  \[
    \mathbb{P}(\text{Reject }\mathcal{H}_0) = \alpha, \quad\text{for } \alpha \in (0,1).
  \]
\end{definition}

\Javier{Note: It is a good practice to write $\forall\,u$ instead of $\forall u$. I modified this in the following equations.}

\begin{definition}[Stochastic dominance]\label{def_sto_def}
  Let $X$ and $Y$ be real-valued random variables.
  We say that $Y$ stochastically dominates $X$, and write
  \[
    X \preceq_{\mathrm{st}} Y,
  \]
  if
  \[
    \mathbb{P}(X \leq u) \geq \mathbb{P}(Y \leq u), \quad \forall\, u \in \mathbb{R}.
  \]
\end{definition}

\Javier{Note: Use $\mathcal{SU}(0,1)$ instead of $SU(0,1)$. I have also replaced it.}

\begin{definition}[Super\added{-}uniform random variable]\label{def_super_unif}
  Let $X$ be a real-valued random variable. We say that $X$ is super\added{-}uniform, and write
  $X \sim \mathcal{SU}(0,1)$, if $X$ stochastically dominates a uniform random
  variable on $[0,1]$, that is, if
  \[
    \mathbb{P}(X \leq u) \leq u, \quad \forall\, u \in [0,1].
  \]
\end{definition}

\begin{proposition}\label{prop_repartition}
  Let $X$ and $Y$ be real-valued random variables, with cumulative distribution functions $F_X$ and $F_Y$, respectively.
  If $X \preceq_{\mathrm{st}} Y$, then $F_X(Y) \sim SU(0,1)$.
\end{proposition}


\begin{remark}\label{remark_FXX_unif}
If \(X=Y\), then \(F_X(X)\) is super-uniform on \([0,1]\).  
Moreover, if \(X\) has a continuous distribution function \(F_X\), then
\[
F_X(X) \sim \mathcal{U}(0,1).
\]
\end{remark}

\Javier{Note: Write $p$-value instead of p-value.}

\begin{definition}[$p$-value, Definition 1.1 in~\cite{Ramdas}]\label{def_pvalue}
  Let $\mathcal{H}_0$ be a hypothesis. A $p$-value for $\mathcal{H}_0$ is a super-uniform random variable under $\mathcal{H}_0$. 
\end{definition}

$p$-values are often used to build a test by defining the partition of the sample sapce using the rejection rule $\mathcal{R} = \1_{\{p \leq \alpha\}}$ for any $\alpha \in (0,1)$.

\begin{proposition}\label{prop_rejection_rule}
  Let $\mathcal{H}_0$ be a hypothesis, $p$ a $p$-value for $\mathcal{H}_0$ and $\mathcal{R}$ the rejection rule defined by
  \[
    \mathcal{R} = \1_{\{p \leq \alpha\}}, \quad \alpha \in (0,1).
  \]
 Then, $\mathcal{R}$
  controls the type I error at level $\alpha$. 
\end{proposition}


\Javier{Here you use the term `rejection rule` (which is fine), that is closely related to `test` (defined above). This should be clarified: either use only test, either use only `rejection rule`, or (better) define `rejection rule` as a particular type of test based on the $p$-value thresholding. After defining $p$-value, you can say that $p$-values are often used to build a test by defining the partition of the sample space using the rejection rule $\mathbb{1}\lbrace p\leq \alpha$, for any $\alpha\in(0,1)$.}

\Javier{\textbf{Importantly}: in the previous proposition you say that the `rejection rule controls the type I error' but the type I error control has been defined in Def. 2.3 for a `test'. My previous comment should help clarify this.}

\begin{definition}[Test statistic]
  A test statistic is a measurable function
  \[
    T : \Omega \to \mathbb{R}
  \]
\end{definition}
From now on, we will consider the case of unilateral tests. In this setting, we define the the $p$-value has the form:
\[
  p(X) = \mathbb{P}_{\mathcal{H}_0}(T(X) \geq t(x))
\]
with $T$, $t$ being test statistics \Javier{what is $T$?}

More generally, the $p$-value for unilateral test will be characterized using test statistics $T$. \Javier{If it is a proposition it can't be a definition! The $p$-value has already been defined in Def. 2.6, so it can't be defined again. What we are doing here is \textit{characterizing} the $p$-value for a unilateral test in terms of a statistic $T$.}

\begin{proposition}\label{prop_test_stat_pvalue}
Let \(T\) and \(T'\) be two test statistics.
\Javier{If $T$ transforms $X$ it can't be taking values from $\mathcal{F}$!}
Let \(X\) be a random variable and \(x\) a realization of \(X\).
Define
\[
p(x) = \mathbb{P}_{\mathcal{H}_0}\!\bigl(T'(X) \ge T(x)\bigr).
\]
If
\[
T'(X) \preceq_{\mathrm{st}} T(X)
\quad \text{under } \mathcal{H}_0,
\]
then \(p\) is a $p$-value for \(\mathcal{H}_0\).
\end{proposition}
\begin{remark}\label{remark_equal_statistics}
If \(T' = T\) and the distribution function \(F_T\) of \(T(X)\) is continuous
under \(\mathcal{H}_0\), then
\[
p \;\overset{\mathcal{H}_0}{\sim}\; \mathcal{U}(0,1).
\]
\end{remark}

\Javier{Here we should have a paragraph introduce what selective testing is. Just saying that in the classical setting the null hypothesis is independent of the data but in several settings $\mathcal{H}_0$ is chosen after seeing the data, what makes the classical testing approaches unsuitable. This is called selective testing, and it is motivated with some examples in the next section. The reader needs to have an idea of what we mean by selective testing before starting reading the examples.}

In the classical setting, the null hypothesis is independent of the data but in several setting $\mathcal{H}_0$ is chosen after seeing the data, what makes the classical testing approaches unsuitable. This is called selective testing, and it is motivated with some examples in the next section.
\subsection{Examples motivating selective testing}

\subsubsection{Lasso}

To determine whether an explanatory variable helps explain a response variable through a linear regression model, a common practice is to test whether its associated coefficient is significantly different from zero.
In the context of classical linear regression, this falls within the framework of non-selective inference.

In contrast, Lasso regression uses an \(\ell_1\)-penalty to perform variable selection. Not all coefficients are tested—only those selected by the Lasso are considered. It is therefore impossible to define in advance which coefficients will be tested, since they depend on the outcome of the Lasso regression. This corresponds to a setting of selective inference.

let \(x_1, \dots, x_8\) be centered gaussian random variables with correlation matrix \((R_{i,j})_{1\le i,j\le 8}\) defined by 

\[
R_{i,j} = 0.5^{|i-j|}, \quad 1\le i,j\le 8.
\]
The response is modeled as
\[
y = \beta^\top x + 3 \varepsilon, \quad \varepsilon \sim \mathcal{N}(0,1), \quad \beta = (3,1.5,0,0,2,0,0,0)^\top.
\]
We generated a sample of size \(n=100\) and obtained the following regularization path.

\Javier{Say instead: to illustrate the unsuitability of classical inference in this context, we simulate samples... peform a LASSO for each sample, etc. You can add in a footnote that this is based on the code by Brian Staber.}

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.7\textwidth]{images/lasso_path.png}
  \caption{Regularization path of the coefficients obtained using the Lasso algorithm.
Some coefficients are zero, and three converge toward the theoretical values 
(3,1.5,2).}
  \label{fig:lasso_path}
\end{figure}

\Javier{Some explanation is needed about the next figure. We tested $\beta_3=$ for each simulated sample and obtained the empirical $p$-value distribution depicted in Figure~\ref{fig:pvalue_lasso}. $p$-values are not super-uniform, therefore...}
We tested $\beta_3 = 0$ for each simulated sample and obtained and obtained the empirical $p$-value distribution depitced in Figure \ref{fig:pvalue_lasso}. $p$-values are not super-uniform, therefore a selective inference approach should be used instead of naive testing after selection.
\begin{figure}[H]
  \centering
  \includegraphics[width = 0.7\textwidth]{images/pvalues_lasso.png}
  \caption{Cumulative distribution function of the $p$-values obtained from significance tests
of the third coefficient estimated by a Lasso regression. The empirical distribution function
was computed using $M$ = 2000 simulations.}
  \label{fig:pvalue_lasso}
\end{figure}


\subsubsection{Publication bias}

Most published studies gain publication due to their demonstrated significance.  
Testing statistical effects from such studies falls within selective inference, as these studies have undergone a selection process.  
If $Y_i \sim \mathcal{N}(\mu_i,1)$ represents the effect size of a scientific study and only those with $|Y_i| > 1$ are published, denoted $\hat{I} = \{i: |Y_i| > 1\}$, then a naive level $\alpha$ test $H_{0,i}: \mu_i = 0$ for $i \in \hat{I}$ is invalid.
Indeed, Fithian \cite{fithian} demonstrates that the false positive rate among true nulls reaches approximately 0.16, far exceeding the nominal 0.05 level.  
Valid inference requires thresholding $|Y_i|$ at 2.41 rather than 1.96, the $0.95$ quantile of the standard normal, imposing a more stringent criterion.

\Javier{Leave the clustering example at the end.}



\subsubsection{Clustering}

Another remarkable example of selective inference appears when evaluating the performance of clustering algorithms by testing for the equality of cluster mean.
To illustrate the need of using appropriate tests in the context of selective inference, we simulated samples of gaussian random variables that were classified into $K = 3$ groups using hierarchical clustering and $k$-means

\Javier{I would start maybe like: Another remarkable example of selective inference appears when evaluating the performace of clustering algorithms by testing for the equality of cluster means (or something like that).}

\Javier{Even if it is only you, avoid using `I` and use `we` or use passive tenses or forms like `we can simulate`, `this can be illustrated by simulating', etc.}

For each sample, equality of cluster means was tested using a classical z-test, for two randomly selected clusters. If the test controlled the type I error in this setting, the resulting $p$-value would be uniformly distributed under the null hypothesis. 
However, the 'clustering + post-selection testing' procedure leads to a deviation from the uniform distribution, demonstrating the lack of control of the type I error, as shown in Figure~\ref{fig:deux_images}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.45\textwidth]{images/cah_pvalues.png}%
  \hfill
  \includegraphics[width=0.45\textwidth]{images/kmeans_df.png}
  \caption{Cumulative distribution functions of the $p$-values obtained from tests of equality of means
    between clusters after hierarchical clustering (CAH) and $k$-means algorithms.
The distribution functions were computed using $M$ = 2000 simulations from a multivariate normal distribution with
  $\mu = 0_{n\times p}$ \Javier{I guess you mean $0_p$} and $\Sigma = 0.98^{|i-j|}$ for $1 \leq i,j \leq n \times p$, \Javier{I don't understand the $n\times p$ here}. with $n = 100$ et $p = 5$. \Javier{Mention that you set the clustering algorithms to choose $K=3$ clusters, and that you tested the equality of cluster means for two randomly selected clusters (if that's the case).}}
  \label{fig:deux_images}
\end{figure}

\Javier{Write $k$-means instead of \textit{k-means}, CAH instead of \textit{CAH} and $p$-value instead of p-value.}

\subsection{Addressing selective testing}

Since the type I error is no longer controlled, alternative approaches are required.
Accordingly, we present three methods as described by Yoav Benjamini \cite{Benjamini}.

\subsubsection{Simultaneous Inference}

This approach controls the family-wise error rate across all hypotheses: 
\[
\mathbb{P}(\text{At least 1 false positive among all hypotheses}) \leq \alpha.
\]

This strategy proves highly conservative, ensuring that for every possible set of hypotheses, the probability of at least one false positive remains below $\alpha$.
\subsubsection{Sample splitting}

This approach consists in splitting the dataset into a training set $X$ and a test set $Y$.  
The training set $X$ is used to choose which hypotheses to test, denoted $H_0(X)$.  
Then, the tests are performed on the test set $Y$.

Although relatively simple to implement, this method raises several issues.  
First, statistical guarantees on the tests hold only if $X$ and $Y$ are independent, which is rarely the case in practice.  
In addition, comparing cluster means on the observations in $Y$ requires assigning each test point to one of the clusters obtained from the clustering performed on $X$, a step that compromises validity.
As discussed in \cite{Gao}, this strategy does not yield valid post-clustering inference in general.


\subsubsection{Conditional Inference}

This approach constitutes the most extensively studied framework for post-clustering inference.  
It controls the false positive rate conditional on hypothesis selection:  
\[
\mathbb{P}(\text{Reject } H_0(X) \mid H_0 \text{ selected}) \leq \alpha.
\]

In the remainder of the article, we employ this method to develop a statistical procedure for testing equality of cluster means following a clustering algorithm.

\section{Post clustering inference}\label{sec:post_clustering_inf}



\subsection{Notation and preliminaries}

Let $X \in \mathbb{R}^{n \times p}$ be the design matrix.
A cluster is an element of a partition of the samples.
We note $C$ a clustering algorithm and $C_1 \in C(X)$ a cluster obtained by the algorithm $C$ on $X$.

We note $\mu = (\mu_{i,j})_{i,j}$ such as $\mu_{i,j} = \mathbb{E}[X_{i,j}]$, with $X_{i,j}$ the element in row i and column j of the matrix $X$.
For a subset $G$ of $\{1,\ldots,n\}$, we note $\overline{\mu}_G = \frac{1}{|G|} \displaystyle\sum_{i \in G} \mu_i \in \mathbb{R}^p$ and $\overline{X}_G = \frac{1}{|G|} \displaystyle\sum_{i \in G} X_i \in \mathbb{R}^p$

\begin{definition}[Null Hypothesis]
  \[H_0^{\{C_1,C_2\}}: \overline{\mu}_{C_1(X)} = \overline{\mu}_{C_2(X)}\]
\end{definition}

\begin{definition}[Type I selective error for clustering]
  We say that a test controls the type I selective error for clustering at leval $\alpha$ if
  \[
    \mathbb{P}_{H_0^{\{C_1,C_2\}}}(Reject ~ H_0^{\{C_1,C_2\}} ~ | ~ C_1,C_2 \in C(X)) \leq \alpha,\quad \alpha \in (0,1).
  \]
  We say that it constrols exactly the type I selective error for clustering at level $\alpha$ if
\[
    \mathbb{P}_{H_0^{\{C_1,C_2\}}}(Reject ~ H_0^{\{C_1,C_2\}} ~ | ~ C_1,C_2 \in C(X)) = \alpha, \quad \alpha \in (0,1).
  \]

\end{definition}

%TODO: Gérer problème d'alinéa

To run a proper test, we need to control this error at level $\alpha$.
In the ideal, we would like to define a p-value as following:
\[
  p_{ideal}(x) = \mathbb{P}_{H_0^{\{C_1,C_2\}}}(T(X) \geq T(x) ~ | ~ C_1,C_2 \in C(X))
\]
with $T$ being a test statistic.

With this p-value, we can control the selective type I error for clustering.

\begin{proposition}\label{prop_pideal}
  The selection rule $\mathcal{R} = \1_{\{p_{ideal} \leq \alpha\}}, \quad \alpha \in (0,1).$ controls the selective type I error for clustering at leval $\alpha$
\end{proposition}

%TODO: proof that p_ideal is a p-value
However, $p_{ideal}$ cannot be evaluated in pratice as it depends on parameters that are unknown. 
Thus, to address this issue, we need to add technical events to the conditioning set and consided:
\[
  p(x) = \mathbb{P}_{H_0^{\{C_1,C_2\}}}(T(X) \geq T(x) ~ | ~ C_1,C_2 \in C(X), E[X])
\]

\subsection{Gao et al. approach}

\appendix

\section{Proofs}

\subsection{Proofs of Section~\ref{sec:testing}}


\begin{proof}[Proof of Proposition~\ref{prop_repartition}]

Let \(G_X\) denote the generalized inverse (quantile function) of \(F_X\),
defined for \(u\in[0,1]\) as
\[
G_X(u) = \inf\{x\in\mathbb{R} : F_X(x) \ge u\}.
\]

By definition of the generalized inverse,
\[
\{F_X(Y) \le u\} = \{Y < G_X(u)\}.
\]
Therefore,
\[
\mathbb{P}\bigl(F_X(Y)\le u\bigr)
= \mathbb{P}\bigl(Y < G_X(u)\bigr)
= F_Y\bigl(G_X(u)^-\bigr),
\]
where \(G_X(u)^-\) denotes the left limit at \(G_X(u)\).

Since \(X \preceq_{\mathrm{st}} Y\), we have \(F_Y \le F_X\) pointwise, and thus
\[
F_Y\bigl(G_X(u)^-\bigr)
\le F_X\bigl(G_X(u)^-\bigr).
\]
By the defining property of the generalized inverse,
\[
F_X\bigl(G_X(u)^-\bigr) \le u.
\]

Combining these inequalities yields
\[
\mathbb{P}\bigl(F_X(Y)\le u\bigr) \le u,
\quad \forall\, u\in[0,1],
\]
which proves that \(F_X(Y)\) is super-uniform.


\end{proof}


\begin{proof}[Proof of Remark~\ref{remark_FXX_unif}]

By the proposition \ref{prop_repartition}, we have that $F_X(X) \sim SU(0,1)$. If \(F_X\) is continuous, then \(F_X(G_X(u)) = u\) for all \(u\in[0,1]\), and hence
\[
\mathbb{P}\bigl(F_X(X)\le u\bigr)
= F_X\bigl(G_X(u)\bigr)
= u,
\quad \forall\,u\in[0,1],
\]
which concludes the proof. \Javier{It's okay, but no need to repeat what we are proving at the end.}

\end{proof}



\begin{proof}[Proof of Proposition~\ref{prop_rejection_rule}]

By definition of the rejection rule,
\[
  \mathbb{P}_{\mathcal{H}_0}(Reject ~ \mathcal{H}_0)
= \mathbb{P}_{\mathcal{H}_0}(p \le \alpha).
\]
Since \(p\) is a $p$-value for \(\mathcal{H}_0\), it is super-uniform under
\(\mathcal{H}_0\), hence
\[
\mathbb{P}_{\mathcal{H}_0}(p \le \alpha) \le \alpha.
\]
This establishes control of the type~I error at level \(\alpha\).
\end{proof}





\begin{proof}[Proof of Proposition~\ref{prop_test_stat_pvalue}]

Let \(F_{T'(X)}\) denote the distribution
function of \(T'(X)\) under $\mathcal{H}_0$.
By Proposition ~\ref{prop_repartition}, the stochastic dominance
\(T'(X) \preceq_{\mathrm{st}} T(X)\) implies that
\[
F_{T'(X)}\bigl(T(X)\bigr) \sim \mathrm{SU}(0,1).
\]

By definition,
\[
p(x)
= \mathbb{P}_{\mathcal{H}_0}\!\bigl(T'(X) \ge T(x)\bigr)
= 1 - F_{T'(X)}\bigl(T(x)\bigr).
\]

Let \(u \in [0,1]\). Then
\begin{align*}
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr)
&= \mathbb{P}_{\mathcal{H}_0}\!\left(1 - F_{T'(X)}(T(X)) \le u\right) \\
&= \mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \ge 1-u\right) \\
&= 1 - \mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \le 1-u\right).
\end{align*}

Since \(F_{T'(X)}(T(X))\) is super-uniform,
\[
\mathbb{P}_{\mathcal{H}_0}\!\left(F_{T'(X)}(T(X)) \le 1-u\right) \le 1-u,
\]
and therefore
\[
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr) \le u.
\]

Thus\added{,} \(p\) is super-uniform under \(\mathcal{H}_0\), and hence a $p$-value.
\end{proof}


\begin{proof}[Proof of Remark~\ref{remark_equal_statistics}]

When \(T'=T\)
\[
p(X) = \mathbb{P}_{\mathcal{H}_0}\bigl(T(X') \ge T(X)\bigr)
      = 1 - F_T\bigl(T(X)\bigr),
\]

By Remark ~\ref{remark_FXX_unif}, if \(F_T\) is continuous,
\[
F_T\bigl(T(X)\bigr) \sim \mathcal{U}(0,1).
\]
Consequently, for any \(u\in[0,1]\),
\begin{align*}
\mathbb{P}_{\mathcal{H}_0}\bigl(p(X) \le u\bigr)
&= \mathbb{P}_{\mathcal{H}_0}\bigl(1 - F_T(T(X)) \le u\bigr) \\
&= \mathbb{P}_{\mathcal{H}_0}\bigl(F_T(T(X)) \ge 1-u\bigr) \\
&= 1 - \mathbb{P}_{\mathcal{H}_0}\bigl(F_T(T(X)) \le 1-u\bigr) \\
&= 1 - (1-u) \\
&= u.
\end{align*}
Thus \(p\) is uniformly distributed on \([0,1]\) under \(\mathcal{H}_0\).
\end{proof}


\subsection{Proofs of Section~\ref{sec:post_clustering_inf}}

\begin{proof}[Proof of Proposition~\ref{prop_pideal}]

  By definition of the rejection rule,
  \[ 
    \mathbb{P}_{H_0^{\{C_1,C_2\}}}\bigl(\text{Reject } H_0^{\{C_1,C_2\}} ~ | ~ C_1,C_2 \in C(X)\bigr)
    =\mathbb{P}_{H_0^{\{C_1,C_2\}}}\bigl(p_{\text{ideal}} \leq \alpha ~ | ~ C_1,C_2 \in C(X)\bigr) 
  \]
  Since $p_{ideal}$ is a p-value for $\mathcal{H}_0^{\{C_1,C_2\}}$, it is super uniform under $\mathcal{H}_0^{\{C_1,C_2\}}$, hence
    \[
    \mathbb{P}_{H_0^{\{C_1,C_2\}}}\bigl(\text{Reject } H_0^{\{C_1,C_2\}} ~ | ~ C_1,C_2 \in C(X)\bigr) \leq \alpha
    \]
  This establishes control of the type I error at leval $\alpha$.
\end{proof}



\bibliographystyle{abbrv}
\bibliography{biblio.bib}

\end{document}
